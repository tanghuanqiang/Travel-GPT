"""
å›¾ç‰‡æœç´¢å·¥å…· - ä½¿ç”¨ Unsplash + Pexels API è·å–çœŸå®æ—…è¡Œç…§ç‰‡

æ”¯æŒçš„ APIï¼š
1. Unsplash API (æ¨èé¦–é€‰) - æ•°ç™¾ä¸‡ä¸“ä¸šæ—…è¡Œç…§ç‰‡
2. Pexels API (å¹¶åˆ—é¦–é€‰) - å®Œå…¨å…è´¹æ— é™è¯·æ±‚

API è·å–æŒ‡å—ï¼š
- Unsplash: https://unsplash.com/developers (å…è´¹ 50 requests/hour)
- Pexels: https://www.pexels.com/api/ (å®Œå…¨å…è´¹æ— é™åˆ¶)
"""
import os
import urllib.parse
import requests
from typing import List, Optional
import time


def get_image_for_activity(activity_name: str, location: str = "", category: str = "") -> List[str]:
    """
    æ ¹æ®æ´»åŠ¨åç§°å’Œä½ç½®è·å–çœŸå®æ™¯ç‚¹å›¾ç‰‡ï¼ˆä¼˜å…ˆä½¿ç”¨ Unsplash/Pexels APIï¼‰
    
    Args:
        activity_name: æ´»åŠ¨åç§°ï¼Œå¦‚"æ•…å®«"ã€"å—ç¿”é¦’å¤´åº—"
        location: ä½ç½®ï¼Œå¦‚"åŒ—äº¬"ã€"ä¸Šæµ·"
        category: ç±»åˆ«ï¼Œå¦‚"æ™¯ç‚¹"ã€"é¤å…"ã€"é…’åº—"
    
    Returns:
        å›¾ç‰‡URLåˆ—è¡¨ï¼ˆ2-3å¼ çœŸå®ç…§ç‰‡ï¼‰ï¼Œå¦‚æœæ‰¾ä¸åˆ°ç›¸å…³å›¾ç‰‡è¿”å›ç©ºåˆ—è¡¨
    """
    import logging
    logger = logging.getLogger(__name__)
    
    logger.info(f"\n{'='*70}")
    logger.info(f"ğŸ” get_image_for_activity è¢«è°ƒç”¨")
    logger.info(f"   è¾“å…¥å‚æ•°:")
    logger.info(f"   - activity_name: {activity_name}")
    logger.info(f"   - location: {location}")
    logger.info(f"   - category: {category}")
    logger.info(f"{'='*70}")
    
    import re
    clean_name = activity_name
    
    # ç§»é™¤å¸¸è§çš„ä¸­æ–‡å‰ç¼€å’ŒåŠ¨è¯
    prefixes_to_remove = [
        r'^æ¸¸è§ˆ[:ï¼š]?',
        r'^å‚è§‚[:ï¼š]?',
        r'^æ‰“å¡[:ï¼š]?',
        r'^ä½“éªŒ[:ï¼š]?',
        r'^æ¢ç´¢[:ï¼š]?',
        r'^åˆé¤[:ï¼š]?',
        r'^æ™šé¤[:ï¼š]?',
        r'^æ—©é¤[:ï¼š]?',
        r'^ç¾é£Ÿ[:ï¼š]?',
        r'^æ–‡åŒ–ä½“éªŒ[:ï¼š]?',
        r'^åˆé¤æ¨è[:ï¼š]?',
        r'^æ™šé¤æ¨è[:ï¼š]?',
        r'^å“å°[:ï¼š]?',
        r'^å‰å¾€[:ï¼š]?',
        r'^åˆ°è¾¾[:ï¼š]?',
        r'^ä¼´æ‰‹ç¤¼é‡‡è´­[:ï¼š]?',
    ]
    
    for pattern in prefixes_to_remove:
        clean_name = re.sub(pattern, '', clean_name, flags=re.IGNORECASE)
    
    # ç§»é™¤æ‹¬å·å†…çš„åº—é“ºä¿¡æ¯ï¼Œå¦‚"ï¼ˆä¸­å¤®å¤§è¡—åº—ï¼‰"
    clean_name = re.sub(r'[ï¼ˆ(][^ï¼‰)]*[åº—é“ºé¦†å…][ï¼‰)]', '', clean_name)
    clean_name = clean_name.strip()
    
    print(f"ğŸ“ æ¸…ç†åçš„åç§°: '{clean_name}'")
    
    # æ™ºèƒ½åˆ†ç±»æ£€æµ‹
    activity_lower = activity_name.lower()
    if not category or category not in ["æ™¯ç‚¹", "é¤å…", "ç¾é£Ÿ", "é…’åº—", "å…¬å›­", "åšç‰©é¦†", "å¯ºåº™", "å¤é•‡", "å¤œæ™¯", "è´­ç‰©"]:
        if any(word in activity_lower for word in ['é¤', 'é¥­', 'åƒ', 'é£Ÿ', 'å…', 'é¦†', 'é“º', 'åŒ…', 'é¥º', 'é¢', 'èœ', 'é”…', 'çƒ¤', 'ç‚–']):
            category = "ç¾é£Ÿ"
        elif any(word in activity_lower for word in ['åšç‰©']):
            category = "åšç‰©é¦†"
        elif any(word in activity_lower for word in ['å¯º', 'åº™', 'å®«', 'æ–‡åº™']):
            category = "å¯ºåº™"
        elif any(word in activity_lower for word in ['å…¬å›­', 'èŠ±å›­']):
            category = "å…¬å›­"
        elif any(word in activity_lower for word in ['è´­ç‰©', 'å•†åœº', 'å•†åŸ', 'ä¸“å–']):
            category = "è´­ç‰©"
        elif any(word in activity_lower for word in ['é…’åº—', 'å®¾é¦†', 'æ°‘å®¿']):
            category = "é…’åº—"
        else:
            category = "æ™¯ç‚¹"
    
    # æ ¹æ®ç±»åˆ«æ„å»ºæ›´æ™ºèƒ½çš„æœç´¢ç­–ç•¥
    def build_search_queries(name: str, loc: str, cat: str) -> List[str]:
        """æ„å»ºå¤šä¸ªæœç´¢æŸ¥è¯¢ï¼Œä»å…·ä½“åˆ°é€šç”¨"""
        queries = []
        
        # ç¾é£Ÿç±»åˆ«ï¼šä½¿ç”¨èœå“ç±»å‹è€Œéé¤å…å
        if cat in ["ç¾é£Ÿ", "é¤å…"]:
            # æå–èœå“å…³é”®è¯
            food_keywords = extract_food_keywords(name, loc)
            if food_keywords:
                queries.append(f"{food_keywords} food dish")
            # æ·»åŠ åœ°æ–¹èœç³»
            if loc:
                regional_cuisine = get_regional_cuisine(loc)
                if regional_cuisine:
                    queries.append(f"{regional_cuisine} cuisine food")
            # é€šç”¨ç¾é£Ÿ
            queries.append(f"Chinese food dish cuisine")
        
        # æ™¯ç‚¹ç±»åˆ«
        elif cat == "æ™¯ç‚¹":
            # å…ˆå°è¯•å…·ä½“æ™¯ç‚¹å + åŸå¸‚
            if loc:
                queries.append(f"{name} {loc} landmark")
            queries.append(f"{name} travel attraction")
            # å¦‚æœæ˜¯è‘—åæ™¯ç‚¹ï¼Œåªç”¨åå­—
            if is_famous_landmark(name):
                queries.append(f"{name}")
        
        # åšç‰©é¦†
        elif cat == "åšç‰©é¦†":
            queries.append(f"{name} museum")
            if loc:
                queries.append(f"{loc} museum gallery")
        
        # å¯ºåº™
        elif cat == "å¯ºåº™":
            queries.append(f"{name} temple")
            if loc:
                queries.append(f"{loc} temple shrine")
            queries.append("Chinese temple architecture")
        
        # è´­ç‰©
        elif cat == "è´­ç‰©":
            if loc:
                queries.append(f"{loc} shopping mall")
            queries.append("shopping mall retail store")
        
        # å…¬å›­
        elif cat == "å…¬å›­":
            queries.append(f"{name} park")
            if loc:
                queries.append(f"{loc} park garden nature")
        
        # é»˜è®¤
        else:
            if loc:
                queries.append(f"{name} {loc}")
            queries.append(f"{name} travel")
        
        return queries
    
    queries = build_search_queries(clean_name, location, category)
    logger.info(f"ğŸ” æœç´¢ç­–ç•¥: {queries}")
    logger.info(f"{'-'*70}")
    
    images = []
    
    # å°è¯•æ¯ä¸ªæŸ¥è¯¢ç›´åˆ°è·å¾—è¶³å¤Ÿçš„å›¾ç‰‡
    for i, query in enumerate(queries):
        if len(images) >= 3:
            break
            
        logger.info(f"\nğŸ“¸ å°è¯•æŸ¥è¯¢ {i+1}/{len(queries)}: '{query}'")
        
        # å…ˆå°è¯• Unsplash
        unsplash_images = search_unsplash(query, count=3 - len(images))
        if unsplash_images:
            images.extend(unsplash_images)
            logger.info(f"   Unsplash: è·å¾— {len(unsplash_images)} å¼ ")
        
        # å¦‚æœè¿˜ä¸å¤Ÿï¼Œå°è¯• Pexels
        if len(images) < 3:
            pexels_images = search_pexels(query, count=3 - len(images))
            if pexels_images:
                # è¿‡æ»¤é‡å¤å›¾ç‰‡
                for img in pexels_images:
                    if img not in images:
                        images.append(img)
                logger.info(f"   Pexels: è·å¾— {len(pexels_images)} å¼ ")
    
    # å»é‡
    images = list(dict.fromkeys(images))
    
    final_count = len(images)
    logger.info(f"\n{'='*70}")
    if images:
        logger.info(f"âœ… æœ€ç»ˆç»“æœ: æˆåŠŸè·å– {final_count} å¼ å›¾ç‰‡")
        for i, img in enumerate(images[:3], 1):
            logger.info(f"   {i}. {img[:100]}...")
    else:
        logger.warning(f"âš ï¸  æœ€ç»ˆç»“æœ: æœªæ‰¾åˆ°ç›¸å…³å›¾ç‰‡ï¼ˆè¿”å›ç©ºæ•°ç»„ï¼‰")
    logger.info(f"{'='*70}\n")
    
    return images[:3] if images else []


def extract_food_keywords(name: str, location: str) -> str:
    """ä»é¤å…åç§°ä¸­æå–èœå“å…³é”®è¯"""
    food_patterns = {
        'é¥ºå­': 'dumplings chinese',
        'åŒ…å­': 'baozi steamed bun',
        'é¦’å¤´': 'mantou steamed bun',
        'æ˜¥é¥¼': 'spring pancake chinese',
        'çƒ¤è‚‰': 'korean bbq grilled meat',
        'ç«é”…': 'hotpot chinese',
        'é“é”…ç‚–': 'stew chinese casserole',
        'ç ‚é”…': 'clay pot stew',
        'è¥¿é¤': 'western food steak',
        'ä¿„ç½—æ–¯': 'russian food cuisine',
        'çº¢è‚ ': 'sausage harbin',
        'é”…åŒ…è‚‰': 'sweet sour pork chinese',
        'å°ç¬¼': 'xiaolongbao soup dumplings',
        'é¢': 'noodles chinese',
        'ç²¥': 'congee rice porridge',
        'çƒ§çƒ¤': 'bbq grilled',
        'æµ·é²œ': 'seafood',
        'å·èœ': 'sichuan spicy food',
        'ç²¤èœ': 'cantonese dim sum',
        'ä¸œåŒ—èœ': 'northeastern chinese food',
    }
    
    for keyword, english in food_patterns.items():
        if keyword in name:
            return english
    
    return ""


def get_regional_cuisine(location: str) -> str:
    """æ ¹æ®åŸå¸‚è·å–åœ°æ–¹èœç³»"""
    cuisine_map = {
        'å“ˆå°”æ»¨': 'northeastern chinese Harbin',
        'ä¸Šæµ·': 'shanghai cuisine',
        'åŒ—äº¬': 'beijing peking food',
        'æˆéƒ½': 'sichuan spicy food',
        'å¹¿å·': 'cantonese dim sum',
        'è¥¿å®‰': 'xian food noodles',
        'é‡åº†': 'chongqing hotpot spicy',
        'æ­å·': 'hangzhou cuisine',
        'å—äº¬': 'jiangsu cuisine',
        'é•¿æ²™': 'hunan spicy food',
    }
    return cuisine_map.get(location, "chinese food")


def is_famous_landmark(name: str) -> bool:
    """æ£€æŸ¥æ˜¯å¦æ˜¯è‘—åæ™¯ç‚¹"""
    famous = [
        'æ•…å®«', 'é•¿åŸ', 'å¤©å®‰é—¨', 'å¤–æ»©', 'ä¸œæ–¹æ˜ç ', 'è¥¿æ¹–', 'å…µé©¬ä¿‘',
        'å¸ƒè¾¾æ‹‰å®«', 'ä¹å¯¨æ²Ÿ', 'é»„å±±', 'å¼ å®¶ç•Œ', 'é¢å’Œå›­', 'å¤©å›',
        'åœ£ç´¢è²äºš', 'ä¸­å¤®å¤§è¡—', 'å¤ªé˜³å²›', 'å†°é›ªå¤§ä¸–ç•Œ',
    ]
    return any(f in name for f in famous)


def get_image_for_location(location: str, image_type: str = "cityscape") -> str:
    """
    è·å–ç›®çš„åœ°çš„åŸå¸‚æ™¯è§‚å›¾ç‰‡ï¼ˆä¼˜å…ˆä½¿ç”¨çœŸå® APIï¼‰
    
    Args:
        location: åŸå¸‚åç§°ï¼Œå¦‚"ä¸Šæµ·"ã€"åŒ—äº¬"
        image_type: å›¾ç‰‡ç±»å‹ï¼Œå¦‚"cityscape"ã€"landscape"ã€"architecture"
    
    Returns:
        å›¾ç‰‡URL
    """
    query = f"{location} {image_type} travel"
    
    # ä¼˜å…ˆ Unsplash
    images = search_unsplash(query, count=1)
    if images:
        return images[0]
    
    # å¤‡ç”¨ Pexels
    images = search_pexels(query, count=1)
    if images:
        return images[0]
    
    # å¦‚æœéƒ½æ²¡æœ‰æ‰¾åˆ°ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²ï¼ˆä¸ä½¿ç”¨å ä½å›¾ï¼‰
    return ""


def search_unsplash(query: str, count: int = 3) -> List[str]:
    """
    ä½¿ç”¨ Unsplash API æœç´¢çœŸå®æ—…è¡Œç…§ç‰‡
    
    è·å– API Keyï¼š
    1. è®¿é—® https://unsplash.com/developers
    2. ç‚¹å‡» "Register as a developer"
    3. åˆ›å»ºåº”ç”¨ (New Application)
    4. å¤åˆ¶ Access Key
    5. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: UNSPLASH_ACCESS_KEY=your_access_key
    
    å…è´¹é™é¢ï¼š50 requests/hourï¼ˆå¯ç”³è¯·æå‡åˆ° 5000/hourï¼‰
    
    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "Eiffel Tower Paris landmark"ï¼‰
        count: è¿”å›å›¾ç‰‡æ•°é‡
    
    Returns:
        å›¾ç‰‡URLåˆ—è¡¨ï¼ˆregular å°ºå¯¸ï¼Œçº¦ 1080pxï¼‰
    """
    import logging
    logger = logging.getLogger(__name__)
    
    logger.debug(f"\nğŸ”µ [Unsplash API] å¼€å§‹è°ƒç”¨")
    logger.debug(f"   æŸ¥è¯¢: '{query}'")
    logger.debug(f"   æ•°é‡: {count}")
    
    api_key = os.getenv("UNSPLASH_ACCESS_KEY")
    if not api_key:
        logger.warning("   âŒ æœªè®¾ç½® UNSPLASH_ACCESS_KEY")
        logger.info("   ğŸ’¡ è¯·è®¿é—® https://unsplash.com/developers è·å–")
        return []
    
    logger.debug(f"   âœ“ API Key å·²é…ç½®: {api_key[:10]}...{api_key[-5:]}")
    
    try:
        url = "https://api.unsplash.com/search/photos"
        headers = {
            "Authorization": f"Client-ID {api_key}"
        }
        params = {
            "query": query,
            "per_page": count,
            "orientation": "landscape",  # æ¨ªå‘å›¾ç‰‡æ›´é€‚åˆæ—…è¡Œå¡ç‰‡
            "content_filter": "high"     # é«˜è´¨é‡è¿‡æ»¤
        }
        
        logger.debug(f"   ğŸ“¡ å‘é€è¯·æ±‚åˆ°: {url}")
        logger.debug(f"   ğŸ“¦ è¯·æ±‚å‚æ•°: {params}")
        
        response = requests.get(url, headers=headers, params=params, timeout=10)
        
        logger.debug(f"   ğŸ“¨ å“åº”çŠ¶æ€ç : {response.status_code}")
        
        response.raise_for_status()
        
        data = response.json()
        results = data.get("results", [])
        total = data.get("total", 0)
        
        logger.debug(f"   ğŸ“Š APIè¿”å›: total={total}, results={len(results)}")
        
        # è¿”å› regular å°ºå¯¸ï¼ˆçº¦1080pxï¼‰ï¼Œé€‚åˆç½‘é¡µæ˜¾ç¤º
        images = [photo["urls"]["regular"] for photo in results]
        
        if images:
            logger.debug(f"   âœ… æˆåŠŸè·å– {len(images)} å¼ å›¾ç‰‡")
            for i, img in enumerate(images[:2], 1):
                logger.debug(f"      {i}. {img[:80]}...")
        else:
            logger.debug(f"   âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡")
        
        return images
    
    except requests.exceptions.Timeout:
        logger.warning(f"   âŒ è¯·æ±‚è¶…æ—¶ (>10ç§’)")
        return []
    except requests.exceptions.RequestException as e:
        logger.error(f"   âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            logger.error(f"   å“åº”å†…å®¹: {e.response.text[:200]}")
        return []
    except Exception as e:
        logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []


def search_pexels(query: str, count: int = 3) -> List[str]:
    """
    ä½¿ç”¨ Pexels API æœç´¢çœŸå®æ—…è¡Œç…§ç‰‡ï¼ˆå®Œå…¨å…è´¹ï¼‰
    
    è·å– API Keyï¼š
    1. è®¿é—® https://www.pexels.com/api/
    2. ç‚¹å‡» "Get Started" æˆ– "Your API Key"
    3. å…è´¹æ³¨å†Œè´¦å·
    4. å¤åˆ¶ API Key
    5. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: PEXELS_API_KEY=your_api_key
    
    å…è´¹é™é¢ï¼š200 requests/hour, 20,000/monthï¼ˆå®Œå…¨å…è´¹ï¼ï¼‰
    
    Args:
        query: æœç´¢å…³é”®è¯ï¼ˆå¦‚ "Grand Palace Bangkok hotel"ï¼‰
        count: è¿”å›å›¾ç‰‡æ•°é‡
    
    Returns:
        å›¾ç‰‡URLåˆ—è¡¨ï¼ˆlarge å°ºå¯¸ï¼‰
    """
    import logging
    logger = logging.getLogger(__name__)
    
    logger.debug(f"\nğŸŸ¢ [Pexels API] å¼€å§‹è°ƒç”¨")
    logger.debug(f"   æŸ¥è¯¢: '{query}'")
    logger.debug(f"   æ•°é‡: {count}")
    
    api_key = os.getenv("PEXELS_API_KEY")
    if not api_key:
        logger.warning("   âŒ æœªè®¾ç½® PEXELS_API_KEY")
        logger.info("   ğŸ’¡ è¯·è®¿é—® https://www.pexels.com/api/ è·å–")
        return []
    
    logger.debug(f"   âœ“ API Key å·²é…ç½®: {api_key[:10]}...{api_key[-5:]}")
    
    try:
        url = "https://api.pexels.com/v1/search"
        headers = {
            "Authorization": api_key
        }
        params = {
            "query": query,
            "per_page": count,
            "orientation": "landscape"
        }
        
        logger.debug(f"   ğŸ“¡ å‘é€è¯·æ±‚åˆ°: {url}")
        logger.debug(f"   ğŸ“¦ è¯·æ±‚å‚æ•°: {params}")
        
        response = requests.get(url, headers=headers, params=params, timeout=10)
        
        logger.debug(f"   ğŸ“¨ å“åº”çŠ¶æ€ç : {response.status_code}")
        
        response.raise_for_status()
        
        data = response.json()
        photos = data.get("photos", [])
        total = data.get("total_results", 0)
        
        logger.debug(f"   ğŸ“Š APIè¿”å›: total_results={total}, photos={len(photos)}")
        
        # è¿”å› large å°ºå¯¸å›¾ç‰‡
        images = [photo["src"]["large"] for photo in photos]
        
        if images:
            logger.debug(f"   âœ… æˆåŠŸè·å– {len(images)} å¼ å›¾ç‰‡")
            for i, img in enumerate(images[:2], 1):
                logger.debug(f"      {i}. {img[:80]}...")
        else:
            logger.debug(f"   âš ï¸  æœªæ‰¾åˆ°å›¾ç‰‡")
        
        return images
    
    except requests.exceptions.Timeout:
        logger.warning(f"   âŒ è¯·æ±‚è¶…æ—¶ (>10ç§’)")
        return []
    except requests.exceptions.RequestException as e:
        logger.error(f"   âŒ ç½‘ç»œè¯·æ±‚å¤±è´¥: {e}")
        if hasattr(e, 'response') and e.response is not None:
            logger.error(f"   å“åº”å†…å®¹: {e.response.text[:200]}")
        return []
    except Exception as e:
        logger.error(f"   âŒ å¤„ç†å¤±è´¥: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return []


def get_placeholder_images(text: str, count: int = 3) -> List[str]:
    """
    è·å–å ä½å›¾ï¼ˆå½“ API ä¸å¯ç”¨æ—¶çš„å¤‡ç”¨æ–¹æ¡ˆï¼‰
    
    Args:
        text: æ˜¾ç¤ºæ–‡æœ¬
        count: å›¾ç‰‡æ•°é‡
    
    Returns:
        å ä½å›¾URLåˆ—è¡¨
    """
    encoded_text = urllib.parse.quote(text[:20])  # é™åˆ¶é•¿åº¦
    base_url = f"https://placehold.co/800x600/3b82f6/ffffff?text={encoded_text}"
    
    # ä½¿ç”¨ä¸åŒé¢œè‰²ç”Ÿæˆå¤šå¼ å ä½å›¾
    colors = ["3b82f6", "8b5cf6", "ec4899", "f59e0b", "10b981"]
    images = []
    
    for i in range(count):
        color = colors[i % len(colors)]
        images.append(f"https://placehold.co/800x600/{color}/ffffff?text={encoded_text}")
    
    return images
